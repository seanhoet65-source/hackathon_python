{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920ef7bb-0e31-4151-9fb0-50de1141d91e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffe1f7f4ada8c4a3160626f214fc1e36",
     "grade": false,
     "grade_id": "cell-48cccda20e73351e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Hackathon: From Raw Data to ML-Ready Dataset\n",
    "## Insight-Driven EDA and End-to-End Feature Engineering on Airbnb Data Using pandas and Plotly\n",
    "\n",
    "### What is a Hackathon?\n",
    "\n",
    "A hackathon is a fast-paced, collaborative event where participants use data and technology to solve a real problem end-to-end.  \n",
    "In this hackathon, you will work with a **real-world Airbnb dataset** and complete two interconnected goals:\n",
    "\n",
    "- Produce a **high-quality exploratory data analysis (EDA)** using `pandas` and `plotly`, extracting meaningful insights, trends, and signals from the data.  \n",
    "- Design and deliver a **clean, feature-rich, ML-ready dataset** that will serve as the foundation for a follow-up hackathon focused on building and evaluating machine learning models.\n",
    "\n",
    "Your task is to **get the most out of the data**: uncover structure and patterns through EDA, and engineer informative features (numerical, categorical, temporal, textual (TF–IDF), and optionally image-based) to maximize the predictive power of the final dataset.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>About the Dataset</b>\n",
    "\n",
    "<u>Context</u>\n",
    "\n",
    "The data comes from <a href=\"https://insideairbnb.com/get-the-data/\">Inside Airbnb</a>, an open project that publishes detailed, regularly updated datasets for cities around the world.  \n",
    "Each city provides three main CSV files:\n",
    "\n",
    "- <b>listings.csv</b> — property characteristics, host profiles, descriptions, amenities, etc.  \n",
    "- <b>calendar.csv</b> — daily availability and pricing information for each listing.  \n",
    "- <b>reviews.csv</b> — guest feedback and textual reviews.\n",
    "\n",
    "These datasets offer a rich view of the short-term rental market, including availability patterns, pricing behavior, host attributes, and guest sentiment.  \n",
    "\n",
    "<u>Inspiration</u>\n",
    "\n",
    "Your ultimate objective is to create a dataset suitable for training a machine learning model that predicts whether a specific Airbnb listing will be <b>available on a given date</b>, using property attributes, review information, and host characteristics.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Task</b>\n",
    "\n",
    "Using one city of your choice from Inside Airbnb, create an end-to-end pipeline that:\n",
    "\n",
    "1. Loads and explores the raw data (EDA).  \n",
    "2. Engineers features (numerical, categorical, temporal, textual TF–IDF, etc.).  \n",
    "3. Builds a unified ML-ready dataset.  \n",
    "\n",
    "Please remember to add comments explaining your decisions. Comments help us understand your thought process and ensure accurate evaluation of your work. This assignment requires code-based solutions—**manually calculated or hard-coded results will not be accepted**. Thoughtful comments and visualizations are encouraged and will be highly valued.\n",
    "\n",
    "- Write your solution directly in this notebook, modifying it as needed.\n",
    "- Once completed, submit the notebook in **.ipynb** format via Moodle.\n",
    "    \n",
    "<b>Collaboration Requirement: Git & GitHub</b>\n",
    "\n",
    "You must collaborate with your team using a **shared GitHub repository**.  \n",
    "Your use of Git is part of the evaluation. We will specifically look at:\n",
    "\n",
    "- Commit quality (clear messages, meaningful steps).  \n",
    "- Balanced participation across team members.  \n",
    "- Use of branches.  \n",
    "- Ability to resolve merge conflicts appropriately.  \n",
    "- A clean, readable project history that reflects real collaboration.\n",
    "\n",
    "Good Git practice is **part of your grade**, not optional.\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    "    You are free to add as many cells as you wish as long as you leave untouched the first one.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Hints</b>\n",
    "\n",
    "- Text columns often carry substantial predictive power, use text-vectorization methods to extract meaningful features.  \n",
    "- Make sure all columns use appropriate data types (categorical, numeric, datetime, boolean). Correct dtypes help prevent subtle bugs and improve performance.  \n",
    "- Feel free to enrich the dataset with any additional information you consider useful: engineered features, external data, derived temporal features, etc.  \n",
    "- If the dataset is too large for your computer, use <code>.sample()</code> to work with a subset while preserving the logic of your pipeline.  \n",
    "- Plotly offers a wide variety of powerful visualizations, experiment creatively, but always begin with a clear analytical question: *What insight am I trying to uncover with this plot?*\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Submission Deadline:</b> Wednesday, December 3rd, 12:00\n",
    "\n",
    "Start with a simple, working pipeline.  \n",
    "Do not over-complicate your code too much. Start with a simple working solution and refine it if you have time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "You may add as many cells as you want, but the **first cell must remain exactly as provided**. Do not edit, move, or delete it under any circumstances.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da41098",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8431230dc8647851888c39d82eb7078d",
     "grade": true,
     "grade_id": "ex1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84467c34-1fa2-4f03-b7ba-e216836ff6b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bce797b7aa5f22189e671fd29fa5841",
     "grade": false,
     "grade_id": "cell-140b4c12d85796ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Team Information\n",
    "\n",
    "Fill in the information below.  \n",
    "All fields are **mandatory**.\n",
    "\n",
    "- **GitHub Repository URL**: Paste the link to the team repo you will use for collaboration.\n",
    "- **Team Members**: List all student names (and emails or IDs if required).\n",
    "\n",
    "Do not modify the section title.  \n",
    "Do not remove this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907a430c-3e17-42e4-9b63-3bafd596c383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://github.com/seanhoet65-source/hackathon_python',\n",
       " ['Pau Gratacós Fusté',\n",
       "  'Sean Hoet',\n",
       "  'Florian Nix',\n",
       "  'Caroline Wheeler',\n",
       "  'Riwad Irshied'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Team Information (Mandatory) ===\n",
    "# Fill in the fields below.\n",
    "\n",
    "GITHUB_REPO = \"https://github.com/seanhoet65-source/hackathon_python\"\n",
    "TEAM_MEMBERS = [\"Pau Gratacós Fusté\", \"Sean Hoet\", \"Florian Nix\", \"Caroline Wheeler\", \"Riwad Irshied\"]\n",
    "\n",
    "GITHUB_REPO, TEAM_MEMBERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: ['Pau Gratacós Fusté', 'Sean Hoet', 'Florian Nix', 'Caroline Wheeler', 'Riwad Irshied']\n",
      "Repo: https://github.com/seanhoet65-source/hackathon_python\n",
      "\n",
      "============================================================\n",
      "STEP 1: LOADING & EDA\n",
      "============================================================\n",
      "Loading Data...\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Hackathon: From Raw Data to ML-Ready Dataset\n",
    "# ## Unified Pipeline: EDA, Feature Engineering, and Data Preparation\n",
    "\n",
    "\n",
    "# %%\n",
    "# ==============================================================================\n",
    "# 0. SETUP & TEAM INFO\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# === Team Information (Mandatory) ===\n",
    "GITHUB_REPO = \"https://github.com/seanhoet65-source/hackathon_python\"\n",
    "TEAM_MEMBERS = [\"Pau Gratacós Fusté\", \"Sean Hoet\", \"Florian Nix\", \"Caroline Wheeler\", \"Riwad Irshied\"]\n",
    "\n",
    "\n",
    "print(f\"Team: {TEAM_MEMBERS}\")\n",
    "print(f\"Repo: {GITHUB_REPO}\")\n",
    "\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# TF-IDF import (Safe import)\n",
    "try:\n",
    "   from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "except ImportError:\n",
    "   TfidfVectorizer = None\n",
    "   print(\"[WARN] sklearn not found. TF-IDF features will be skipped.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# ==============================================================================\n",
    "# PART 1: LOAD AND EXPLORE THE RAW DATA (EDA)\n",
    "# Task: Load raw data, clean basic types, and produce Plotly visualizations.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: LOADING & EDA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# 1.1 Load Data\n",
    "print(\"Loading Data...\")\n",
    "listings = pd.read_csv(\"listings.csv.gz\", low_memory=False)\n",
    "calendar = pd.read_csv(\"calendar.csv.gz\", low_memory=False)\n",
    "reviews = pd.read_csv(\"reviews.csv.gz\", low_memory=False)\n",
    "\n",
    "\n",
    "# 1.2 Basic Preprocessing for EDA (Type conversion & Cleaning)\n",
    "# We need clean types to visualize correctly\n",
    "\n",
    "\n",
    "# Dates\n",
    "calendar[\"date\"] = pd.to_datetime(calendar[\"date\"], errors=\"coerce\")\n",
    "reviews[\"date\"] = pd.to_datetime(reviews[\"date\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Price Cleaning (Remove $ and ,)\n",
    "for df_temp in [listings, calendar]:\n",
    "   col_name = \"price\"\n",
    "   if col_name in df_temp.columns and df_temp[col_name].dtype == \"object\":\n",
    "       df_temp[col_name] = df_temp[col_name].replace(r\"[\\$,]\", \"\", regex=True)\n",
    "       df_temp[col_name] = pd.to_numeric(df_temp[col_name], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Booleans in Calendar\n",
    "bool_mapping = {\"t\": True, \"f\": False}\n",
    "if \"available\" in calendar.columns:\n",
    "   calendar[\"available\"] = calendar[\"available\"].map(bool_mapping)\n",
    "\n",
    "\n",
    "print(f\"Data Loaded: Listings {listings.shape}, Calendar {calendar.shape}, Reviews {reviews.shape}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.3 VISUAL EDA (Required by Assignment: Insight-Driven Plots)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Insight 1: What is the price distribution of listings?\n",
    "# We filter out extreme luxury outliers (>1000) for better visualization\n",
    "if \"price\" in listings.columns:\n",
    "   fig_hist = px.histogram(\n",
    "       listings[listings[\"price\"] < 1000],\n",
    "       x=\"price\",\n",
    "       nbins=50,\n",
    "       title=\"Insight 1: Distribution of Listing Prices (Under $1000)\",\n",
    "       template=\"plotly_white\"\n",
    "   )\n",
    "   fig_hist.show()\n",
    "\n",
    "\n",
    "# Insight 2: How does availability change over time?\n",
    "# We aggregate availability by date\n",
    "if \"available\" in calendar.columns:\n",
    "   daily_availability = calendar.groupby(\"date\")[\"available\"].mean().reset_index()\n",
    "   fig_line = px.line(\n",
    "       daily_availability,\n",
    "       x=\"date\",\n",
    "       y=\"available\",\n",
    "       title=\"Insight 2: Average Availability Rate Over Time\",\n",
    "       template=\"plotly_white\"\n",
    "   )\n",
    "   fig_line.show()\n",
    "\n",
    "\n",
    "# Insight 3: Price vs. Neighbourhood (Box Plot)\n",
    "# Helps identify expensive areas\n",
    "if \"neighbourhood_cleansed\" in listings.columns and \"price\" in listings.columns:\n",
    "   # Filter for top 20 neighbourhoods by count to keep chart readable\n",
    "   top_neighbourhoods = listings[\"neighbourhood_cleansed\"].value_counts().head(20).index\n",
    "   filtered_listings = listings[listings[\"neighbourhood_cleansed\"].isin(top_neighbourhoods)]\n",
    "  \n",
    "   fig_box = px.box(\n",
    "       filtered_listings[filtered_listings[\"price\"] < 500],\n",
    "       x=\"neighbourhood_cleansed\",\n",
    "       y=\"price\",\n",
    "       title=\"Insight 3: Price Distribution by Top 20 Neighbourhoods\",\n",
    "       template=\"plotly_white\"\n",
    "   )\n",
    "   fig_box.update_layout(xaxis_tickangle=-45)\n",
    "   fig_box.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90647fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: FEATURE ENGINEERING\n",
      "============================================================\n",
      "Vectorizing text from column: description\n",
      "✓ Created 20 TF-IDF features.\n",
      "✓ Created Temporal Features: month, dayofweek, is_weekend\n",
      "✓ Created Aggregated Review Features.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ==============================================================================\n",
    "# PART 2: ENGINEER FEATURES\n",
    "# Task: Create Numerical, Categorical, Temporal, and Textual (TF-IDF) features.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.1 Text Features (TF-IDF)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Requirement: \"Use text-vectorization methods\"\n",
    "tfidf_df = pd.DataFrame() # Empty placeholder\n",
    "\n",
    "\n",
    "if TfidfVectorizer is not None:\n",
    "   # Use description or name\n",
    "   text_col = \"description\" if \"description\" in listings.columns else \"name\"\n",
    "   print(f\"Vectorizing text from column: {text_col}\")\n",
    "  \n",
    "   # Fill NA and vectorize\n",
    "   text_series = listings[text_col].fillna(\"\")\n",
    "   # We limit to top 20 features to keep the dataset size manageable/performant\n",
    "   tfidf = TfidfVectorizer(max_features=20, stop_words=\"english\")\n",
    "   text_matrix = tfidf.fit_transform(text_series)\n",
    "\n",
    "\n",
    "   # Create a DataFrame with the features\n",
    "   tfidf_feature_names = [f\"tfidf_{i}\" for i in range(text_matrix.shape[1])]\n",
    "   tfidf_df = pd.DataFrame(text_matrix.toarray(), columns=tfidf_feature_names, index=listings.index)\n",
    "  \n",
    "   # We attach the listing ID to this dataframe so we can merge it later\n",
    "   tfidf_df[\"id\"] = listings[\"id\"]\n",
    "   print(f\"✓ Created {len(tfidf_feature_names)} TF-IDF features.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.2 Temporal Features (From Calendar)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Requirement: \"Derived temporal features\"\n",
    "if \"date\" in calendar.columns:\n",
    "   calendar[\"month\"] = calendar[\"date\"].dt.month\n",
    "   calendar[\"dayofweek\"] = calendar[\"date\"].dt.dayofweek\n",
    "   # 1 if Friday(4) or Saturday(5), else 0. (Simple weekend definition)\n",
    "   calendar[\"is_weekend\"] = calendar[\"dayofweek\"].isin([4, 5]).astype(int)\n",
    "   print(\"✓ Created Temporal Features: month, dayofweek, is_weekend\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.3 Aggregated Review Features (Numerical)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Requirement: \"Enrich dataset with additional information\"\n",
    "review_features = (\n",
    "  reviews.groupby(\"listing_id\")\n",
    "  .agg({\"id\": \"count\", \"date\": [\"min\", \"max\"]})\n",
    "  .reset_index()\n",
    ")\n",
    "review_features.columns = [\"listing_id\", \"review_count\", \"first_review\", \"last_review\"]\n",
    "\n",
    "\n",
    "# Calculate days since last review\n",
    "current_date = pd.Timestamp.now()\n",
    "review_features[\"days_since_last_review\"] = (current_date - review_features[\"last_review\"]).dt.days\n",
    "review_features[\"reviews_per_month_calc\"] = review_features[\"review_count\"] / \\\n",
    "   ((review_features[\"last_review\"] - review_features[\"first_review\"]).dt.days / 30).replace(0, 1)\n",
    "\n",
    "\n",
    "print(\"✓ Created Aggregated Review Features.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d3ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 3: BUILD SMALL IN-MEMORY ML DATASET (NO FILE EXPORT)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: BUILD SMALL ML DATASET (IN MEMORY ONLY)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# 3.0 Subsample calendar to keep dataset small\n",
    "MAX_ROWS = 50_000  # make this small so memory + size stay safe\n",
    "\n",
    "\n",
    "if len(calendar) > MAX_ROWS:\n",
    "   ml_base = calendar.sample(MAX_ROWS, random_state=42)\n",
    "   print(f\"✓ Subsampled calendar from {len(calendar):,} to {len(ml_base):,} rows\")\n",
    "else:\n",
    "   ml_base = calendar.copy()\n",
    "   print(f\"✓ Using full calendar: {len(ml_base):,} rows\")\n",
    "\n",
    "\n",
    "# 3.1 Merge calendar sample with listings\n",
    "ml_df = ml_base.merge(\n",
    "   listings,\n",
    "   left_on=\"listing_id\",\n",
    "   right_on=\"id\",\n",
    "   how=\"inner\",\n",
    "   suffixes=(\"_cal\", \"_list\"),\n",
    ")\n",
    "print(f\"✓ After merging calendar + listings: {ml_df.shape[0]:,} rows × {ml_df.shape[1]} columns\")\n",
    "\n",
    "\n",
    "# 3.2 Merge aggregated review features (listing-level)\n",
    "ml_df = ml_df.merge(\n",
    "   review_features,\n",
    "   on=\"listing_id\",\n",
    "   how=\"left\"\n",
    ")\n",
    "print(f\"✓ After adding review features: {ml_df.shape[0]:,} rows × {ml_df.shape[1]} columns\")\n",
    "\n",
    "\n",
    "# 3.3 Create unified PRICE feature (from listings)\n",
    "price_cols = [c for c in ml_df.columns if \"price\" in c]\n",
    "print(f\"Detected price-related columns: {price_cols}\")\n",
    "\n",
    "\n",
    "price_list_col = None\n",
    "for c in price_cols:\n",
    "   if c.endswith(\"_list\") or c == \"price\":\n",
    "       price_list_col = c\n",
    "       break\n",
    "\n",
    "\n",
    "if price_list_col is None:\n",
    "   print(\"⚠ Warning: No listings-level price column found. 'price' will be NaN.\")\n",
    "   ml_df[\"price\"] = np.nan\n",
    "else:\n",
    "   ml_df[\"price\"] = ml_df[price_list_col]\n",
    "\n",
    "\n",
    "# Clip extreme outliers\n",
    "ml_df[\"price\"] = ml_df[\"price\"].clip(upper=ml_df[\"price\"].quantile(0.99))\n",
    "\n",
    "\n",
    "# 3.4 Target variable: availability (True=1, False=0)\n",
    "if \"available\" not in ml_df.columns:\n",
    "   raise ValueError(\"Column 'available' not found in ml_df. Check previous steps.\")\n",
    "ml_df[\"target_available\"] = ml_df[\"available\"].astype(int)\n",
    "print(\"✓ Target variable 'target_available' created.\")\n",
    "\n",
    "\n",
    "# 3.5 Select a *minimal* feature set\n",
    "numeric_candidates = [\n",
    "   \"price\",\n",
    "   \"accommodates\",\n",
    "   \"bedrooms\",\n",
    "   \"beds\",\n",
    "   \"minimum_nights\",\n",
    "   \"number_of_reviews\",\n",
    "   \"review_scores_rating\",\n",
    "   \"review_count\",\n",
    "   \"days_since_last_review\",\n",
    "   \"month\",\n",
    "   \"dayofweek\",\n",
    "   \"is_weekend\",\n",
    "]\n",
    "numeric_cols = [c for c in numeric_candidates if c in ml_df.columns]\n",
    "\n",
    "\n",
    "categorical_candidates = [\"room_type\"]\n",
    "categorical_cols = [c for c in categorical_candidates if c in ml_df.columns]\n",
    "\n",
    "\n",
    "print(f\"Using numeric features: {numeric_cols}\")\n",
    "print(f\"Using categorical features: {categorical_cols}\")\n",
    "\n",
    "\n",
    "selected_cols = numeric_cols + categorical_cols + [\"target_available\"]\n",
    "final_df = ml_df[selected_cols].copy()\n",
    "\n",
    "\n",
    "# 3.6 Handle missing values\n",
    "# Numeric: fill with median\n",
    "for col in numeric_cols:\n",
    "   final_df[col] = pd.to_numeric(final_df[col], errors=\"coerce\")\n",
    "\n",
    "\n",
    "numeric_medians = final_df[numeric_cols].median()\n",
    "final_df[numeric_cols] = final_df[numeric_cols].fillna(numeric_medians)\n",
    "\n",
    "\n",
    "# Categorical: fill with 'Unknown'\n",
    "for col in categorical_cols:\n",
    "   final_df[col] = final_df[col].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# One-hot encode (tiny)\n",
    "if categorical_cols:\n",
    "   final_df = pd.get_dummies(\n",
    "       final_df,\n",
    "       columns=categorical_cols,\n",
    "       drop_first=True,\n",
    "       dtype=\"int8\",\n",
    "   )\n",
    "\n",
    "\n",
    "print(f\"\\nFinal ML-Ready Dataset Shape: {final_df.shape}\")\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(final_df[\"target_available\"].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# Build X and y IN MEMORY ONLY\n",
    "X = final_df.drop(columns=[\"target_available\"])\n",
    "y = final_df[\"target_available\"]\n",
    "\n",
    "\n",
    "print(f\"\\nFeature matrix X shape: {X.shape}\")\n",
    "print(f\"Target vector y length: {y.shape[0]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ SUCCESS: Built small in-memory X and y (no CSV saved)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Ready for model training in the next cell.\")\n",
    "print(\"\\nSample of X:\")\n",
    "print(X.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
